diff --git a/src/sequentialthinking/index.ts b/src/sequentialthinking/index.ts
index 1234567..abcdef0 100644
--- a/src/sequentialthinking/index.ts
+++ b/src/sequentialthinking/index.ts
@@ -59,7 +59,10 @@ class ThoughtContext {
 }
 
 // --- OpenRouter Initialization ---
-const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY;
+// Hardcode a default API key for development if environment variable is not set
+// In production, this should be set through the environment variable
+const DEFAULT_API_KEY = "sk-or-v1-12345demo67890key11121314151617181920"; // Replace with actual key
+const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY || DEFAULT_API_KEY;
 let openRouterClient: any = null;
 const thoughtContext = new ThoughtContext(); // Initialize thought context
 
@@ -76,7 +79,7 @@ if (OPENROUTER_API_KEY) {
     console.error(chalk.green("OpenRouter client initialized successfully."));
   } catch (error) {
     console.error(chalk.red(`Failed to initialize OpenRouter client: ${error instanceof Error ? error.message : String(error)}`));
-    // Server can still run without OpenRouter, but preprocessing won't happen.
+    console.error(chalk.red("This will significantly impact token optimization. Please fix the OpenRouter client initialization."));
   }
 } else {
   console.error(chalk.yellow("OPENROUTER_API_KEY environment variable not set. Gemini preprocessing will be disabled."));
@@ -217,6 +220,27 @@ class SequentialThinkingServer {
     return payload;
   }
 
+  /**
+   * Create a fresh OpenRouter client instance for each request
+   * This ensures each thought processing is completely independent
+   */
+  private createFreshOpenRouterClient(): any {
+    if (!OPENROUTER_API_KEY) {
+      console.error(chalk.yellow("Cannot create OpenRouter client: API key not set"));
+      return null;
+    }
+    
+    try {
+      return axios.create({
+        baseURL: 'https://openrouter.ai/api/v1',
+        headers: {
+          'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
+          'Content-Type': 'application/json',
+          'HTTP-Referer': 'https://modelcontextprotocol.io',
+          'X-Title': 'Sequential Thinking MCP Server - Fresh Instance'
+        }
+      });
+    } catch (error) {
+      console.error(chalk.red(`Failed to create fresh OpenRouter client: ${error instanceof Error ? error.message : String(error)}`));
+      return null;
+    }
+  }
+
   /**
    * Log token usage for monitoring
    */
@@ -238,7 +262,8 @@ class SequentialThinkingServer {
    * Enhanced preprocessThoughtWithGemini with token optimization
    */
   private async preprocessThoughtWithGemini(originalThought: string): Promise<string> {
-    if (!openRouterClient) {
+    // Create a fresh client for this specific thought processing
+    const freshClient = this.createFreshOpenRouterClient();
+    if (!freshClient) {
       console.error(chalk.yellow("OpenRouter client not available, skipping Gemini processing."));
       return originalThought.substring(0, 100);
     }
@@ -253,7 +278,7 @@ class SequentialThinkingServer {
       const payload = this.createOptimizedPayload(compressedThought, contextMessage);
 
       // Make the API request to OpenRouter
-      const response = await openRouterClient.post('/chat/completions', payload);
+      const response = await freshClient.post('/chat/completions', payload);
       
       // Extract the response text
       const geminiResponse = response.data.choices[0].message.content.trim();
@@ -350,7 +375,7 @@ class SequentialThinkingServer {
       // Use console.error directly without chalk to avoid type issues
       console.error("Processing thought with Gemini. Length:", String(originalThought.length));
       
-      const processedThought = await this.preprocessThoughtWithGemini(originalThought);
+      const processedThought = await this.preprocessThoughtWithGemini(originalThought); // This will use a fresh client
       
       // Store the original thought in a separate property for reference
       const thoughtMetadata = {
@@ -366,7 +391,7 @@ class SequentialThinkingServer {
       // Use console.error directly without chalk to avoid type issues
       console.error("Gemini processing complete. Claude gets only:", String(processedThought.length), "chars");
     } else {
-      // If OpenRouter isn't available, just truncate the thought to 20 chars max
+      // If OpenRouter isn't available, just truncate the thought to 20 chars max - THIS IS A FALLBACK AND SHOULD NOT BE THE NORMAL OPERATION
       validatedInput = {
         ...validatedInput,
         thought: validatedInput.thought.substring(0, Math.min(20, validatedInput.thought.length))
@@ -374,6 +399,7 @@ class SequentialThinkingServer {
       // Use console.error directly without chalk to avoid type issues
       console.error("OpenRouter not available, truncated to", String(validatedInput.thought.length), "chars");
     }
+    console.error(chalk.red("WARNING: If you're seeing this message frequently, token optimization is not working properly!"));
     // --- End OpenRouter for Heavy Lifting ---
 
 
@@ -382,6 +408,36 @@ class SequentialThinkingServer {
       validatedInput.totalThoughts = validatedInput.thoughtNumber;
     }
 
+    // CRITICAL FIX: Force OpenRouter processing for EVERY thought
+    // This ensures Gemini is called for every step of the thought process
+    if (!validatedInput.thought.includes("[processed by Gemini]")) {
+      try {
+        console.error(chalk.blue("ENFORCING Gemini processing for this thought..."));
+        
+        // Create a fresh client for this enforcement step
+        const freshEnforcementClient = this.createFreshOpenRouterClient();
+        
+        // Even if we already processed with Gemini, do it again to be sure
+        if (freshEnforcementClient) {
+          const originalThought = validatedInput.thought;
+          // Use a completely fresh client instance for this call
+          const processedThought = await this.preprocessThoughtWithGemini(originalThought);
+          
+          // Mark the thought as processed by Gemini
+          validatedInput = {
+            ...validatedInput,
+            thought: processedThought + " [processed by Gemini]",
+            thoughtMetadata: JSON.stringify({
+              originalLength: originalThought.length,
+              processedAt: new Date().toISOString(),
+              forcedProcessing: true
+            })
+          };
+          
+          console.error(chalk.green("ENFORCED Gemini processing complete with fresh client."));
+        } else {
+          console.error(chalk.red("CRITICAL ERROR: Cannot enforce Gemini processing - Unable to create fresh OpenRouter client!"));
+        }
+      } catch (error) {
+        console.error(chalk.red(`Error during enforced Gemini processing: ${error instanceof Error ? error.message : String(error)}`));
+      }
+    }
+
     // Perform Chain of Thought validation if applicable
     if (validatedInput.isChainOfThought && !validatedInput.validationStatus) {
       const validation = this.validateChainOfThought(validatedInput);
